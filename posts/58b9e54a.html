<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/M-apple.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/M-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/M-16x16.png"><link rel="mask-icon" href="/images/M.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"miraclesky.cn",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!0},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="Pytorch从零构建ResNet50第二章  从零构建ResNet50 前言 ResNet 目前是应用很广的网络基础框架，所以有必要了解一下，并且resnet结构清晰，适合练手. 有了前面resnet18的经验，现在搭建50就没那么难了，如果没有看，请移步 第一章  从零构建ResNet18  本文使用以下环境构筑 123torch 1.11torchvision 0.12.0python"><meta property="og:type" content="article"><meta property="og:title" content="Pytorch从零构建ResNet50"><meta property="og:url" content="https://miraclesky.cn/posts/58b9e54a.html"><meta property="og:site_name" content="You make yourself what you are"><meta property="og:description" content="Pytorch从零构建ResNet50第二章  从零构建ResNet50 前言 ResNet 目前是应用很广的网络基础框架，所以有必要了解一下，并且resnet结构清晰，适合练手. 有了前面resnet18的经验，现在搭建50就没那么难了，如果没有看，请移步 第一章  从零构建ResNet18  本文使用以下环境构筑 123torch 1.11torchvision 0.12.0python"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic4.zhimg.com/80/v2-252e6d9979a2a91c2d3033b9b73eb69f_720w.jpg"><meta property="og:image" content="https://s2.loli.net/2022/05/21/1GEv6eNlHFM4UxW.png"><meta property="og:image" content="https://s2.loli.net/2022/05/21/8wd9uoWHRbiLJNv.png"><meta property="og:image" content="https://s2.loli.net/2022/05/21/YwSGEh6iLtHznyF.png"><meta property="og:image" content="https://img-blog.csdnimg.cn/056d487e245f405c96503b7f23a9a869.png"><meta property="og:image" content="https://img-blog.csdnimg.cn/bbed1d892d204750b863e833a6474963.png"><meta property="article:published_time" content="2022-05-23T15:12:15.000Z"><meta property="article:modified_time" content="2022-05-23T15:12:15.000Z"><meta property="article:author" content="Miracle"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="ResNet50"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic4.zhimg.com/80/v2-252e6d9979a2a91c2d3033b9b73eb69f_720w.jpg"><link rel="canonical" href="https://miraclesky.cn/posts/58b9e54a.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>Pytorch从零构建ResNet50 | You make yourself what you are</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="You make yourself what you are" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">You make yourself what you are</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">欲上层楼 哪料山外青山楼为楼</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i> 公益 404</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div> <a href="https://github.com/Mirac1eSky" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="external nofollow noopener noreferrer" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"/></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://miraclesky.cn/posts/58b9e54a.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/my.jpg"><meta itemprop="name" content="Miracle"><meta itemprop="description" content="也无风雨也无晴"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="You make yourself what you are"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> Pytorch从零构建ResNet50</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-05-23 23:12:15" itemprop="dateCreated datePublished" datetime="2022-05-23T23:12:15+08:00">2022-05-23</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:inline-flex"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>19k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>48 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><script type="text/javascript" src="/js/baidu.js"></script><script type="text/javascript" src="/js/360.js"></script><h1 id="Pytorch从零构建ResNet50"><a href="#Pytorch从零构建ResNet50" class="headerlink" title="Pytorch从零构建ResNet50"></a>Pytorch从零构建ResNet50</h1><p>第二章 从零构建ResNet50</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><ul><li>ResNet 目前是应用很广的网络基础框架，所以有必要了解一下，并且resnet结构清晰，适合练手.</li><li><p><strong>有了前面resnet18的经验，现在搭建50就没那么难了，如果没有看，请移步 <a href="https://blog.csdn.net/weixin_39524208/article/details/124894216" target="_blank" rel="external nofollow noopener noreferrer">第一章 从零构建ResNet18</a></strong></p></li><li><p>本文使用以下环境构筑</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch <span class="number">1.11</span></span><br><span class="line">torchvision <span class="number">0.12</span><span class="number">.0</span></span><br><span class="line">python <span class="number">3.9</span></span><br></pre></td></tr></table></figure></li></ul><hr><a id="more"></a><h1 id="一、Res50和Res18的区别？"><a href="#一、Res50和Res18的区别？" class="headerlink" title="一、Res50和Res18的区别？"></a>一、Res50和Res18的区别？</h1><h2 id="1-残差块的区别"><a href="#1-残差块的区别" class="headerlink" title="1. 残差块的区别"></a>1. 残差块的区别</h2><ul><li><p>如下图</p><p><img src="https://pic4.zhimg.com/80/v2-252e6d9979a2a91c2d3033b9b73eb69f_720w.jpg" alt="BasicBlock"></p></li></ul><p>​ 这种跳跃连接就叫做<strong>shortcut connection</strong>（类似电路中的短路）。上面这种两层结构的叫<strong>BasicBlock</strong>，一般适用于<strong>ResNet18</strong>和<strong>ResNet34</strong>。而<strong>ResNet50</strong>以后都使用下面这种三层的残差结构叫<strong>Bottleneck</strong></p><p><img src="https://s2.loli.net/2022/05/21/1GEv6eNlHFM4UxW.png" alt="Bottleneck"></p><p>最明显的区别就是，<strong>Bottleneck</strong>中有三层，中间层是kernel为3的卷积层，一头一尾则是kernel为1的卷积，明显是用来做通道数的变换。</p><ul><li>残差块个数的变化<br>刚才说的是残差块内部的个数变化，现在这个指的是，整个残差块数量的变化，很容易理解，就不多说了。</li></ul><h2 id="2-ResNet50具体结构"><a href="#2-ResNet50具体结构" class="headerlink" title="2. ResNet50具体结构"></a>2. ResNet50具体结构</h2><p>上面简单说了一下ResNet1818和ResNet50的区别，接下来用图来看一下ResNet50网络的结构<img src="https://s2.loli.net/2022/05/21/8wd9uoWHRbiLJNv.png" alt="resnet结构"></p><p>可以看到，50层的网络也是有五个部分组成，从conv2开始，每层都有不同个数的残差块，分别为3，4，6，3. 接下来再具体50层的具体结构</p><p><img src="https://s2.loli.net/2022/05/21/YwSGEh6iLtHznyF.png" alt="res18&amp;50"></p><p>其中，<strong>蓝色</strong>部分为<strong>conv2</strong>,然后往下依次按颜色划分为conv3、conv4，conv5。</p><blockquote><p>需要注意的地方有以下几点:</p><ul><li>与18层不同的是，50层的ResNet在第一次池化之后，就需要将<strong>X</strong>升维，也就是虚线对应的部分，不可以直接相加</li><li><strong>Bottleneck</strong>的最后一层是进行升维操作的，所以在几个相同<strong>Bottleneck</strong>堆叠的时候，需要注意输入维度的转变。例如conv2有三个<strong>Bottleneck</strong>，将这三个编号为a，b，c，则这三个的输入输出分别为<ul><li>a的输入维度：64，64，64，输出维度：64，64，256</li><li>b的输入维度：256，64，64，输出维度：64，64，256</li><li>c的输入维度：256，64，64，输出维度：64，64，256</li></ul></li></ul></blockquote><h1 id="二、ResNet分步骤实现"><a href="#二、ResNet分步骤实现" class="headerlink" title="二、ResNet分步骤实现"></a>二、ResNet分步骤实现</h1><blockquote><p>首先实现残差块：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,in_channels,out_channels,stride=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],first=False)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        super(Bottleneck,self).__init__()</span><br><span class="line">        self.bottleneck = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels,out_channels,kernel_size=<span class="number">1</span>,stride=stride[<span class="number">0</span>],padding=padding[<span class="number">0</span>],bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>), <span class="comment"># 原地替换 节省内存开销</span></span><br><span class="line">            nn.Conv2d(out_channels,out_channels,kernel_size=<span class="number">3</span>,stride=stride[<span class="number">1</span>],padding=padding[<span class="number">1</span>],bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>), <span class="comment"># 原地替换 节省内存开销</span></span><br><span class="line">            nn.Conv2d(out_channels,out_channels*<span class="number">4</span>,kernel_size=<span class="number">1</span>,stride=stride[<span class="number">2</span>],padding=padding[<span class="number">2</span>],bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels*<span class="number">4</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># shortcut 部分</span></span><br><span class="line">        <span class="comment"># 由于存在维度不一致的情况 所以分情况</span></span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> first:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                <span class="comment"># 卷积核为1 进行升降维</span></span><br><span class="line">                <span class="comment"># 注意跳变时 都是stride==2的时候 也就是每次输出信道升维的时候</span></span><br><span class="line">                nn.Conv2d(in_channels, out_channels*<span class="number">4</span>, kernel_size=<span class="number">1</span>, stride=stride[<span class="number">1</span>], bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(out_channels*<span class="number">4</span>)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.bottleneck(x)</span><br><span class="line">        out += self.shortcut(x)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><blockquote><p>接下来是ResNet50的具体实现<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用bn的网络中，卷积层的输出并不加偏置</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet50</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,Bottleneck, num_classes=<span class="number">10</span>)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        super(ResNet50, self).__init__()</span><br><span class="line">        self.in_channels = <span class="number">64</span></span><br><span class="line">        <span class="comment"># 第一层作为单独的 因为没有残差快</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">64</span>,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding=<span class="number">3</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># conv2</span></span><br><span class="line">        self.conv2 = self._make_layer(Bottleneck,<span class="number">64</span>,[[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]*<span class="number">3</span>,[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]]*<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># conv3</span></span><br><span class="line">        self.conv3 = self._make_layer(Bottleneck,<span class="number">128</span>,[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>]] + [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]*<span class="number">3</span>,[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]]*<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># conv4</span></span><br><span class="line">        self.conv4 = self._make_layer(Bottleneck,<span class="number">256</span>,[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>]] + [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]*<span class="number">5</span>,[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]]*<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># conv5</span></span><br><span class="line">        self.conv5 = self._make_layer(Bottleneck,<span class="number">512</span>,[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>]] + [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]*<span class="number">2</span>,[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]]*<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.fc = nn.Linear(<span class="number">2048</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self,block,out_channels,strides,paddings)</span>:</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="comment"># 用来判断是否为每个block层的第一层</span></span><br><span class="line">        flag = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(strides)):</span><br><span class="line">            layers.append(block(self.in_channels,out_channels,strides[i],paddings[i],first=flag))</span><br><span class="line">            flag = <span class="literal">False</span></span><br><span class="line">            self.in_channels = out_channels * <span class="number">4</span></span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.conv4(out)</span><br><span class="line">        out = self.conv5(out)</span><br><span class="line"></span><br><span class="line">        out = self.avgpool(out)</span><br><span class="line">        out = out.reshape(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><br>可以输出网络结构看一下<p></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res50 = ResNet50(Bottleneck)</span><br><span class="line">print(res50)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br></pre></td><td class="code"><pre><span class="line">ResNet50(</span><br><span class="line">  (conv1): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">7</span>, <span class="number">7</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">3</span>, <span class="number">3</span>), bias=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">1</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (conv2): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">64</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">64</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">256</span>, <span class="number">64</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">64</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential()</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">2</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">256</span>, <span class="number">64</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">64</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (conv3): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">256</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">128</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">128</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential()</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">2</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">128</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential()</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">3</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">128</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (conv4): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">512</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">256</span>, <span class="number">1024</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">1024</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">512</span>, <span class="number">1024</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">1024</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">256</span>, <span class="number">1024</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">1024</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential()</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">2</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">256</span>, <span class="number">1024</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">1024</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential()</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">3</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">256</span>, <span class="number">1024</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">1024</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential()</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">4</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">256</span>, <span class="number">1024</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">1024</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential()</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">5</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">256</span>, <span class="number">1024</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">1024</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (conv5): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">1024</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">512</span>, <span class="number">2048</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">2048</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">1024</span>, <span class="number">2048</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">2048</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">2048</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">512</span>, <span class="number">2048</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">2048</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential()</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">2</span>): Bottleneck(</span><br><span class="line">      (bottleneck): Sequential(</span><br><span class="line">        (<span class="number">0</span>): Conv2d(<span class="number">2048</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">1</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">2</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">3</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">4</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">5</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">6</span>): Conv2d(<span class="number">512</span>, <span class="number">2048</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=<span class="literal">False</span>)</span><br><span class="line">        (<span class="number">7</span>): BatchNorm2d(<span class="number">2048</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">      (shortcut): Sequential()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (fc): Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>至此，Resnet50就构建好了</p><h1 id="三、完整例子-测试"><a href="#三、完整例子-测试" class="headerlink" title="三、完整例子+测试"></a>三、完整例子+测试</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, utils</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataset <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span> <span class="comment"># 控制显示</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([ToTensor(),</span><br><span class="line">                                transforms.Normalize(</span><br><span class="line">                                    mean=[<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>],</span><br><span class="line">                                    std=[<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>]</span><br><span class="line">                                ),</span><br><span class="line">                                transforms.Resize((<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">                               ])</span><br><span class="line"></span><br><span class="line">training_data = datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">"data"</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=transform,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">testing_data = datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">"data"</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=transform,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,in_channels,out_channels,stride=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],first=False)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        super(Bottleneck,self).__init__()</span><br><span class="line">        self.bottleneck = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels,out_channels,kernel_size=<span class="number">1</span>,stride=stride[<span class="number">0</span>],padding=padding[<span class="number">0</span>],bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>), <span class="comment"># 原地替换 节省内存开销</span></span><br><span class="line">            nn.Conv2d(out_channels,out_channels,kernel_size=<span class="number">3</span>,stride=stride[<span class="number">1</span>],padding=padding[<span class="number">1</span>],bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>), <span class="comment"># 原地替换 节省内存开销</span></span><br><span class="line">            nn.Conv2d(out_channels,out_channels*<span class="number">4</span>,kernel_size=<span class="number">1</span>,stride=stride[<span class="number">2</span>],padding=padding[<span class="number">2</span>],bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels*<span class="number">4</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># shortcut 部分</span></span><br><span class="line">        <span class="comment"># 由于存在维度不一致的情况 所以分情况</span></span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> first:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                <span class="comment"># 卷积核为1 进行升降维</span></span><br><span class="line">                <span class="comment"># 注意跳变时 都是stride==2的时候 也就是每次输出信道升维的时候</span></span><br><span class="line">                nn.Conv2d(in_channels, out_channels*<span class="number">4</span>, kernel_size=<span class="number">1</span>, stride=stride[<span class="number">1</span>], bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(out_channels*<span class="number">4</span>)</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># if stride[1] != 1 or in_channels != out_channels:</span></span><br><span class="line">        <span class="comment">#     self.shortcut = nn.Sequential(</span></span><br><span class="line">        <span class="comment">#         # 卷积核为1 进行升降维</span></span><br><span class="line">        <span class="comment">#         # 注意跳变时 都是stride==2的时候 也就是每次输出信道升维的时候</span></span><br><span class="line">        <span class="comment">#         nn.Conv2d(in_channels, out_channels*4, kernel_size=1, stride=stride[1], bias=False),</span></span><br><span class="line">        <span class="comment">#         nn.BatchNorm2d(out_channels)</span></span><br><span class="line">        <span class="comment">#     )</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.bottleneck(x)</span><br><span class="line">        out += self.shortcut(x)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用bn的网络中，卷积层的输出并不加偏置</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet50</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,Bottleneck, num_classes=<span class="number">10</span>)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        super(ResNet50, self).__init__()</span><br><span class="line">        self.in_channels = <span class="number">64</span></span><br><span class="line">        <span class="comment"># 第一层作为单独的 因为没有残差快</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">64</span>,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding=<span class="number">3</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># conv2</span></span><br><span class="line">        self.conv2 = self._make_layer(Bottleneck,<span class="number">64</span>,[[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]*<span class="number">3</span>,[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]]*<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># conv3</span></span><br><span class="line">        self.conv3 = self._make_layer(Bottleneck,<span class="number">128</span>,[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>]] + [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]*<span class="number">3</span>,[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]]*<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># conv4</span></span><br><span class="line">        self.conv4 = self._make_layer(Bottleneck,<span class="number">256</span>,[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>]] + [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]*<span class="number">5</span>,[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]]*<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># conv5</span></span><br><span class="line">        self.conv5 = self._make_layer(Bottleneck,<span class="number">512</span>,[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>]] + [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]*<span class="number">2</span>,[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]]*<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.fc = nn.Linear(<span class="number">2048</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self,block,out_channels,strides,paddings)</span>:</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="comment"># 用来判断是否为每个block层的第一层</span></span><br><span class="line">        flag = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(strides)):</span><br><span class="line">            layers.append(block(self.in_channels,out_channels,strides[i],paddings[i],first=flag))</span><br><span class="line">            flag = <span class="literal">False</span></span><br><span class="line">            self.in_channels = out_channels * <span class="number">4</span></span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.conv4(out)</span><br><span class="line">        out = self.conv5(out)</span><br><span class="line"></span><br><span class="line">        out = self.avgpool(out)</span><br><span class="line">        out = out.reshape(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 保持数据集和测试机能完整划分</span></span><br><span class="line">batch_size=<span class="number">64</span></span><br><span class="line">train_data = DataLoader(dataset=training_data,batch_size=batch_size,shuffle=<span class="literal">True</span>,drop_last=<span class="literal">True</span>)</span><br><span class="line">test_data = DataLoader(dataset=testing_data,batch_size=batch_size,shuffle=<span class="literal">True</span>,drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">images,labels = next(iter(train_data))</span><br><span class="line">print(images.shape)</span><br><span class="line">img = utils.make_grid(images)</span><br><span class="line">img = img.numpy().transpose(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)</span><br><span class="line">mean=[<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>]</span><br><span class="line">std=[<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>]</span><br><span class="line">img = img * std + mean</span><br><span class="line">print([labels[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">64</span>)])</span><br><span class="line">plt.imshow(img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">model = res18.to(device)</span><br><span class="line">cost = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line">print(len(train_data))</span><br><span class="line">print(len(test_data))</span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    running_correct = <span class="number">0.0</span></span><br><span class="line">    model.train()</span><br><span class="line">    print(<span class="string">"Epoch &#123;&#125;/&#123;&#125;"</span>.format(epoch+<span class="number">1</span>,epochs))</span><br><span class="line">    print(<span class="string">"-"</span>*<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> X_train,y_train <span class="keyword">in</span> train_data:</span><br><span class="line">        <span class="comment"># X_train,y_train = torch.autograd.Variable(X_train),torch.autograd.Variable(y_train)</span></span><br><span class="line">        X_train,y_train = X_train.to(device), y_train.to(device)</span><br><span class="line">        outputs = model(X_train)</span><br><span class="line">        _,pred = torch.max(outputs.data,<span class="number">1</span>)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss = cost(outputs,y_train)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        running_correct += torch.sum(pred == y_train.data)</span><br><span class="line"></span><br><span class="line">    testing_correct = <span class="number">0</span></span><br><span class="line">    test_loss = <span class="number">0</span></span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="keyword">for</span> X_test,y_test <span class="keyword">in</span> test_data:</span><br><span class="line">        <span class="comment"># X_test,y_test = torch.autograd.Variable(X_test),torch.autograd.Variable(y_test)</span></span><br><span class="line">        X_test,y_test = X_test.to(device), y_test.to(device)</span><br><span class="line">        outputs = model(X_test)</span><br><span class="line">        loss = cost(outputs,y_test)</span><br><span class="line">        _,pred = torch.max(outputs.data,<span class="number">1</span>)</span><br><span class="line">        testing_correct += torch.sum(pred == y_test.data)</span><br><span class="line">        test_loss += loss.item()</span><br><span class="line">    print(<span class="string">"Train Loss is:&#123;:.4f&#125;, Train Accuracy is:&#123;:.4f&#125;%, Test Loss is::&#123;:.4f&#125; Test Accuracy is:&#123;:.4f&#125;%"</span>.format(</span><br><span class="line">        running_loss/len(training_data), <span class="number">100</span>*running_correct/len(training_data),</span><br><span class="line">        test_loss/len(testing_data),</span><br><span class="line">        <span class="number">100</span>*testing_correct/len(testing_data)</span><br><span class="line">    ))</span><br></pre></td></tr></table></figure><blockquote><p>数据的可视化，是没有问题的</p></blockquote><p><img src="https://img-blog.csdnimg.cn/056d487e245f405c96503b7f23a9a869.png" alt="数据集"></p><blockquote><p>然后看看跑的结果，这里因为配置原因，只跑了五轮</p></blockquote><p><img src="https://img-blog.csdnimg.cn/bbed1d892d204750b863e833a6474963.png" alt="结果"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过手写ResNet18和50，可以加深对残差网络的理解，同时运用pytorch更加熟练。</p><blockquote><p>PS: 此博客同时更新于<a href="https://blog.csdn.net/weixin_39524208/article/details/124928837" target="_blank" rel="external nofollow noopener noreferrer">CSDN</a></p></blockquote></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">------------------已经触及底线啦<i class="fa fa-paw"></i>感谢您的阅读------------------</div></div></div><div class="reward-container"><div></div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechat.jpg" alt="Miracle 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="Miracle 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> Miracle</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://miraclesky.cn/posts/58b9e54a.html" title="Pytorch从零构建ResNet50">https://miraclesky.cn/posts/58b9e54a.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/atom.xml"><span class="icon"><i class="fa fa-rss"></i></span> <span class="label">RSS</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/ResNet/" rel="tag"><i class="fa fa-tag"></i> ResNet</a><a href="/tags/Pytorch/" rel="tag"><i class="fa fa-tag"></i> Pytorch</a></div><div class="post-widgets"><div class="wp_rating"><div id="wpac-rating"></div></div></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/77df9efe.html" rel="prev" title="Pytorch从零构建ResNet18"><i class="fa fa-chevron-left"></i> Pytorch从零构建ResNet18</a></div><div class="post-nav-item"></div></div></footer></article></div><div class="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC80OTUwOS8yNjAwMA"></div></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Pytorch从零构建ResNet50"><span class="nav-number">1.</span> <span class="nav-text">Pytorch从零构建ResNet50</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#前言"><span class="nav-number">2.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#一、Res50和Res18的区别？"><span class="nav-number">3.</span> <span class="nav-text">一、Res50和Res18的区别？</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-残差块的区别"><span class="nav-number">3.1.</span> <span class="nav-text">1. 残差块的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-ResNet50具体结构"><span class="nav-number">3.2.</span> <span class="nav-text">2. ResNet50具体结构</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#二、ResNet分步骤实现"><span class="nav-number">4.</span> <span class="nav-text">二、ResNet分步骤实现</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#三、完整例子-测试"><span class="nav-number">5.</span> <span class="nav-text">三、完整例子+测试</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#总结"><span class="nav-number">6.</span> <span class="nav-text">总结</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Miracle" src="/images/my.jpg"><p class="site-author-name" itemprop="name">Miracle</p><div class="site-description" itemprop="description">也无风雨也无晴</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">14</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">15</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/Mirac1eSky" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Mirac1eSky" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="external nofollow noopener noreferrer" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2022</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Miracle</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">120k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">4:59</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:inline-flex"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:inline-flex"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script>CONFIG.page.isPost&&(wpac_init=window.wpac_init||[],wpac_init.push({widget:"Rating",id:24411,el:"wpac-rating",color:"fc6423"}),function(){if(!("WIDGETPACK_LOADED"in window)){WIDGETPACK_LOADED=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//embed.widgetpack.com/widget.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t.nextSibling)}}())</script><script src="/js/local-search.js"></script><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/hijiki.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0},react:{opacity:.7}})</script></body></html>