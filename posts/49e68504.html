<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/M-apple.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/M-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/M-16x16.png"><link rel="mask-icon" href="/images/M.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"miraclesky.cn",root:"/",scheme:"Muse",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!0},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="[Deep Learning] 浅层神经网络 理解隐藏层和隐藏单元 能够使用多种类型的激活函数 用隐藏层建立一个前向传播和反向传播算法 应用随机初始化"><meta property="og:type" content="article"><meta property="og:title" content="neural-networks-deep-learning-week3"><meta property="og:url" content="http://miraclesky.cn/posts/49e68504.html"><meta property="og:site_name" content="醉后凉风起，吹人舞袖回"><meta property="og:description" content="[Deep Learning] 浅层神经网络 理解隐藏层和隐藏单元 能够使用多种类型的激活函数 用隐藏层建立一个前向传播和反向传播算法 应用随机初始化"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://i.loli.net/2020/04/08/BVQtXdyYAgEn2pc.png"><meta property="og:image" content="https://i.loli.net/2020/03/31/LlMgaBbvCmTZnyr.png"><meta property="og:image" content="https://i.loli.net/2020/03/31/hzbAeauWGZRItFM.png"><meta property="og:image" content="https://i.loli.net/2020/03/31/A3i1mjGnDJQ58YV.png"><meta property="og:image" content="https://i.loli.net/2020/03/31/YEVe8Xv9My4WQZ3.png"><meta property="og:image" content="https://i.loli.net/2020/03/31/FcsIWydzCPEST6k.png"><meta property="article:published_time" content="2020-03-30T09:50:10.000Z"><meta property="article:modified_time" content="2020-03-30T09:50:10.000Z"><meta property="article:author" content="Miracle"><meta property="article:tag" content="人工智能"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://i.loli.net/2020/04/08/BVQtXdyYAgEn2pc.png"><link rel="canonical" href="http://miraclesky.cn/posts/49e68504.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>neural-networks-deep-learning-week3 | 醉后凉风起，吹人舞袖回</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="醉后凉风起，吹人舞袖回" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">醉后凉风起，吹人舞袖回</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">唯见月寒日暖</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i> 公益 404</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div> <a href="https://github.com/Mirac1eSky" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="external nofollow noopener noreferrer" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"/></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://miraclesky.cn/posts/49e68504.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/my.jpg"><meta itemprop="name" content="Miracle"><meta itemprop="description" content="也无风雨也无晴"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="醉后凉风起，吹人舞袖回"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> neural-networks-deep-learning-week3</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-03-30 17:50:10" itemprop="dateCreated datePublished" datetime="2020-03-30T17:50:10+08:00">2020-03-30</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:inline-flex"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>2.9k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>7 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><script type="text/javascript" src="/js/baidu.js"></script><script type="text/javascript" src="/js/360.js"></script><h1 id="Deep-Learning-浅层神经网络"><a href="#Deep-Learning-浅层神经网络" class="headerlink" title="[Deep Learning] 浅层神经网络"></a>[Deep Learning] 浅层神经网络</h1><ul><li>理解隐藏层和隐藏单元</li><li>能够使用多种类型的激活函数</li><li>用隐藏层建立一个前向传播和反向传播算法</li><li>应用随机初始化</li></ul><a id="more"></a><h2 id="浅层神经网络结构"><a href="#浅层神经网络结构" class="headerlink" title="浅层神经网络结构"></a>浅层神经网络结构</h2><ul><li><h3 id="单样本前向传播"><a href="#单样本前向传播" class="headerlink" title="单样本前向传播"></a>单样本前向传播</h3><p>首先，看一下神经网络的结构图</p><p><img src="https://i.loli.net/2020/04/08/BVQtXdyYAgEn2pc.png" alt></p></li></ul><p>这是一个两层的神经网络（输入层不算做一层，认为是第0层）。一般用 <strong>X</strong> 代表输入，也可以用<script type="math/tex">a^{[0]}</script> 表示 （a代 表激活的意思），用<script type="math/tex">a^{[1]}</script>代表第一层即隐藏层，<script type="math/tex">a^{[2]}</script>代表输出层。所以上图中 ，<strong>X</strong> =<script type="math/tex">a^{[0]}</script><script type="math/tex">\in R^{3}</script> ,<script type="math/tex">a^{[1]} \in R^{4}</script>,<script type="math/tex">a^{[2]}\in R</script> 。 现在来看隐藏层第一个节点即<script type="math/tex">a^{[1]}_{1}</script>的计算，这个计算和逻辑回归一样。首先算出</p><script type="math/tex;mode=display">
z^{[1]}_1 = w^{[1]T}_1x + b^{[1]}_1</script><p> 然后计算</p><script type="math/tex;mode=display">
a^{[1]}_1 = sigmoid(z^{[1]}_1)</script><p> 其他节点 同理计算，整理一下</p><script type="math/tex;mode=display">
z^{[1]}_1 = w^{[1]T}_1x + b^{[1]}_1,a^{[1]}_1 = sigmoid(z^{[1]}_1)</script><script type="math/tex;mode=display">
z^{[1]}_2 = w^{[1]T}_2x + b^{[1]}_2,a^{[1]}_2=sigmoid(z^{[1]}_1)</script><script type="math/tex;mode=display">
z^{[1]}_3 = w^{[1]T}_3x + b^{[1]}_3,a^{[1]}_3=sigmoid(z^{[1]}_3)</script><script type="math/tex;mode=display">
z^{[1]}_4 = w^{[1]T}_4x + b^{[1]}_4,a^{[1]}_4=sigmoid(z^{[1]}_4)</script><ul><li><h3 id="前向传播向量化"><a href="#前向传播向量化" class="headerlink" title="前向传播向量化"></a>前向传播向量化</h3><p>接下来将这几个式子向量化</p></li></ul><script type="math/tex;mode=display">
\left[
 \begin{matrix}
   --w^{[1]T}_1-- \\
   --w^{[1]T}_2-- \\
   --w^{[1]T}_3-- \\
   --w^{[1]T}_4-- 
  \end{matrix}
  \right] 
*
\left[
\begin{matrix}
   x_1 \\
   x_2 \\
   x_3

\end{matrix}
\right]   + 
\left[
\begin{matrix}
b^{1}_1 \\
b^{1}_2 \\
b^{1}_3 \\
b^{1}_4

\end{matrix}
\right]
=
\left[
\begin{matrix}
w^{[1]T}_1x + b^{[1]}_1 \\
w^{[1]T}_2x + b^{[1]}_2 \\
w^{[1]T}_3x + b^{[1]}_3 \\
w^{[1]T}_4x + b^{[1]}_4 

\end{matrix}

\right]
=
\left[
\begin{matrix}
 z^{[1]}_1 \\
 z^{[1]}_2 \\
 z^{[1]}_3 \\
 z^{[1]}_4 \\
\end{matrix}
\right]</script><p>​ 第一个矩阵为4 x 3的矩阵 ,这是因为<script type="math/tex">w^{1}_l</script>都属于 三维列向量，现在将他们的转置竖直拼 接为新的矩阵，所以维度4x3。简化一下，可以得到下面的公式</p><script type="math/tex;mode=display">
z^{[1]} = W^{[1]} x + b^{[1]}</script><script type="math/tex;mode=display">
a^{[1]} = sigmoid(z^{[1]})</script><script type="math/tex;mode=display">
z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}</script><script type="math/tex;mode=display">
a^{[2]} = sigmoid(z^{[2]})</script><p>​ 因为 x 可以写作<script type="math/tex">a^{[0]}</script>,所以</p><script type="math/tex;mode=display">
z^{[1]} = W^{[1]} a^{[0]} + b^{[1]}</script><p>​ 其中，<script type="math/tex">W^{[1]} \in R^{4*3},b\in R^{4*1}, W^{[2]}\in R^{1*4},b^{[2]} \in R</script>,所以最后<script type="math/tex">a^{[2]}</script> 也是一个实数。</p><p>​ 到此，就完成了一个实例的神经网络计算。</p><ul><li><h3 id="多样本前向传播"><a href="#多样本前向传播" class="headerlink" title="多样本前向传播"></a>多样本前向传播</h3></li></ul><p> 接下来 讨论一下对于多个样本的计算：</p><p> 首先解释一下符号的意思， 第<strong>i</strong>个样本第<strong>k</strong>层的节点定义为<script type="math/tex">a^{[k](i)}</script>。</p><p> 则在计算m个样本的时候，需要像下面这样计算</p><p><strong>for i = 1 to m:</strong></p><script type="math/tex;mode=display">
z^{[1](i)} = W^{[1]} x^{(i)} + b^{[1]}</script><script type="math/tex;mode=display">
a^{[1](i)} = sigmoid(z^{[1](i)})</script><script type="math/tex;mode=display">
z^{[2](i)} = W^{[2]}a^{[1](i)} + b^{[2]}</script><script type="math/tex;mode=display">
a^{[2](i)} = sigmoid(z^{[2](i)})</script><ul><li><h3 id="多样本前向传播向量化"><a href="#多样本前向传播向量化" class="headerlink" title="多样本前向传播向量化"></a>多样本前向传播向量化</h3><p>首先将多个x样本水平拼接为<strong>X</strong>，维度为 n x m ，即</p></li></ul><script type="math/tex;mode=display">
  X = 
  \left[
  \begin{matrix}
  | & |  &  | \\
  x^{(1)} & x^{(2)} & x^{(3)} \\
  | & |  &  | \\

  \end{matrix} \tag{n * m}
  \right]</script><p> 则 向量化 如下</p><script type="math/tex;mode=display">
  Z^{[1]} =  W^{[1]} X + b^{[1]}</script><script type="math/tex;mode=display">
A^{[1]} = sigmoid(Z^{[1]})</script><script type="math/tex;mode=display">
  Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}</script><script type="math/tex;mode=display">
  A^{[2]} = sigmoid(Z^{[2]})</script><p> 同样的 ，<script type="math/tex">Z^{[1]}</script> ，<script type="math/tex">A^{[1]}</script>都是矩阵。到此就已经全部完成了。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ul><li><h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p>目前为止，我们使用的激活函数一直都是sigmoid。它的图像是这样：</p></li></ul><p><img src="https://i.loli.net/2020/03/31/LlMgaBbvCmTZnyr.png" style="zoom:50%"></p><p>可以看到，当z非常大或者非常小的时候，它斜率也非常小，这样会使梯度下降很慢。所以基本上不会使用这个激活函数。 <font color="red">除非在做二元分类的时候，需要输出值在o-1之间,只有在这个时候会在输出层选择它。需要注意的是，对于隐藏层和输出层来讲，可以选择不同的激活函数。</font></p><ul><li><h3 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h3><p>一般情况下，tanh函数都要优于sigmoid函数，它的图像如下：</p><p><img src="https://i.loli.net/2020/03/31/hzbAeauWGZRItFM.png" style="zoom:50%"></p><p>它的公式为：</p></li></ul><script type="math/tex;mode=display">
tanh = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}</script><p>其实是sigmoid函数经过变换后得来的。尽管他的表现要比sigmoid函数要好，但是仍旧面临同样的问题，就是当z过大或过小的时候，斜率非常小。所以也不怎么使用它。</p><ul><li><h3 id="ReLU函数（线性修正单元）"><a href="#ReLU函数（线性修正单元）" class="headerlink" title="ReLU函数（线性修正单元）"></a>ReLU函数（线性修正单元）</h3><p>它的公式为 <strong>ReLU = max(0,z)</strong> , 图像长这样：</p><p><img src="https://i.loli.net/2020/03/31/A3i1mjGnDJQ58YV.png" style="zoom:50%"></p></li></ul><p>所以只要z为正数，那么斜率就一直为1，z为负数时，斜率为0。虽然当z=0时，导数无定义（左导数和右导数不相等），但是在实际编程时，遇到z=0的概率非常非常小，并且也可以通过指定z=0时，导数为0或者1，所以不用纠结这个问题。</p><p>所以一般情况下都用<strong>ReLU</strong>，或者在你不知道使用哪种激活函数时，就选择它。就比如我们在做二元分类的问题时，可以将所有隐藏层的激活函数都设置为<strong>ReLU</strong>，然后将输出层的激活函数设置为<strong>sigmoid</strong>。</p><p>此外 ，ReLU函数还有另一种版本。</p><ul><li><h3 id="Leaky-ReLU（带泄露的ReLU）"><a href="#Leaky-ReLU（带泄露的ReLU）" class="headerlink" title="Leaky ReLU（带泄露的ReLU）"></a>Leaky ReLU（带泄露的ReLU）</h3></li></ul><p>它的公式为 <strong>ReLU = max(0.01z,z)</strong> , 图像长这样：</p><p><img src="https://i.loli.net/2020/03/31/YEVe8Xv9My4WQZ3.png" style="zoom:50%"></p><p>当z为负数时，它的斜率不在为0，而是一个轻微的倾斜。这种激活函数实际上使用起来和ReLU没什么太大的区别，具体可以看个人爱好，或者在你的模型中，将两种都尝试一下，来试试效果。</p><p>顺带说一句，因为在NN中，有足够多的隐藏单元来使得z为正数，所以在使用ReLU的时候也不用太担心梯度下降的问题。</p><h2 id="激活函数求导"><a href="#激活函数求导" class="headerlink" title="激活函数求导"></a>激活函数求导</h2><ul><li><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><p>关于它的求导，可以参考上一篇博客：<a href="https://www.miraclesky.cn/neural-networks-deep-learning-week2.html" target="_blank" rel="noopener">neural-networks-deep-learning-week2</a></p><p>这里就不再赘述。</p></li><li><h3 id="tanh-函数"><a href="#tanh-函数" class="headerlink" title="tanh 函数"></a>tanh 函数</h3><p>首先，有它的公式</p><script type="math/tex;mode=display">
g(z) = tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}</script></li></ul><p> 则有</p><script type="math/tex;mode=display">
  g^{'}（z） = 1 - (tanh(z))^{2}</script><p> 具体求导过程可以自行求导，不是很难</p><ul><li><h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><script type="math/tex;mode=display">
g(z) = max(0,z)</script><p>求导非常简单</p><script type="math/tex;mode=display">
g^{'}(z) = 
\left\{ 
\begin{aligned}
1 & &if && z\geq0 \\
0 & &if && z<0
\end{aligned}
\right.</script><p>当z=0时，我们手动设置导数为0或者1（在数学中这是不可以的，==该点不可导==）</p></li></ul><h2 id="神经网络中的梯度下降"><a href="#神经网络中的梯度下降" class="headerlink" title="神经网络中的梯度下降"></a><font color="red">神经网络中的梯度下降</font></h2><p>现在需要讨论 在NN 中如何进行梯度下降，假设目前还是做得二元分类问题且只包含一层隐藏层，那么我们有损失函数的定义如下：</p><script type="math/tex;mode=display">
J(w^{[1]},b^{[1]},w^{[2]},b^{[2]}) =\frac{1}{m}\sum_{i=1}^m  L(\hat{y}^\left(i\right),y^\left(i\right)) \\
其中 ，n_x=n^{[0]}为特征的个数，n^{[1]}为隐藏单元个数，n^{[2]}为输出单元个数\\
w^{[1]}的维度 为 n^{[1]} * n^{[0]}，b^{[1]}是一个n^{[1]}维向量，即n^{[1]}*1 \\
w^{[1]}的维度 为 n^{[2]} * n^{[1]}，b^{[2]}是一个n^{[2]}维向量，即n^{[2]}*1</script><ul><li>梯度下降的做法就是用 变量 减去 学习率与对应偏导数的乘积</li></ul><script type="math/tex;mode=display">
\begin{array}\\
Repeat\{\\
dw^{[1]} = \frac{\partial J}{\partial w^{[1]}}, db^{[1]} = \frac{\partial J}{\partial b^{[1]}} …… \\
w^{[1]} = w^{[1]} - \alpha * dw^{[1]},b^{[1]} = b^{[1]} - \alpha * db^{[1]},……\\
\}
\end{array}</script><ul><li><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>具体的过程为（向量化）：</p></li></ul><script type="math/tex;mode=display">
\begin{array}
dZ^{[2]} = A^{[2]} - Y \\
dW^{[2]} = \frac{1}{m}dZ^{[2]} * A^{[1]T} \\
db^{[2]} =  \frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)\\
axis=1为水平方向，keepdims=True防止输出秩为一的矩阵即(n,)这种形式 \\
dZ^{[1]} = W^{[2]T}  dZ^{[2]} .* g^{[1]'}(Z^{[1]})              \\
dW^{[1]} = \frac{1}{m} dZ^{[1]}X^{T}\\
db^{[1]} = \frac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True)\\
\end{array}</script> <font color="red">（ .* 代表逐个元素相乘 ）</font><p>整个过程就是这样，如果不是很明白，可以去B站搜一搜，有很多推导过程，由于时间所限，这里就不一一推导。</p><h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>在逻辑回归中，将权重参数全部初始化为0这是可以的，但是在NN中这是不行的。我们来看一看这是为什么。</p><p><img src="https://i.loli.net/2020/03/31/FcsIWydzCPEST6k.png" style="zoom:50%"></p><p>在上图中，有两个输入即<script type="math/tex">n^{[0]} =2</script>, 隐藏层有两个单元即<script type="math/tex">n^{[1]} =2</script>,那么<script type="math/tex">W^{[1]}</script>是一个2x2的矩阵，<script type="math/tex">b^{[1]}</script>是一个2x1的矩阵。而这会导致无论输入是什么，总有<script type="math/tex">a^{[1]}_1 =</script><script type="math/tex">a^{[1]}_2</script>。当进行反向传播的时候，由于对称问题，会导致<script type="math/tex">dz^{[1]}_1</script> =<script type="math/tex">dz^{[1]}_2</script></p><p>……</p><p>因此，如果全部初始化为0的话，所以的隐藏单元实际上都是相同的，无论训练多久，它们始终相同。推广到多个隐藏层，隐藏单元也依旧成立。所以需要随机初始化。</p><hr><p>本周内容相对而言稍微难一点，主要是反向传播中的链式求导，不太熟的可以去看看链式求导法则。最好是要搞懂原理，不然在之后的多层网络中会比较难以理解</p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">------------------已经触及底线啦<i class="fa fa-paw"></i>感谢您的阅读------------------</div></div></div><div class="reward-container"><div></div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechat.jpg" alt="Miracle 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="Miracle 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> Miracle</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="http://miraclesky.cn/posts/49e68504.html" title="neural-networks-deep-learning-week3">http://miraclesky.cn/posts/49e68504.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/atom.xml"><span class="icon"><i class="fa fa-rss"></i></span> <span class="label">RSS</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> 神经网络</a></div><div class="post-widgets"><div class="wp_rating"><div id="wpac-rating"></div></div></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/3ee1b592.html" rel="prev" title="neural-networks-deep-learning-week2"><i class="fa fa-chevron-left"></i> neural-networks-deep-learning-week2</a></div><div class="post-nav-item"> <a href="/posts/d78210a7.html" rel="next" title="neural-networks-deep-learning-week4">neural-networks-deep-learning-week4<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC80OTUwOS8yNjAwMA"></div></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Deep-Learning-浅层神经网络"><span class="nav-number">1.</span> <span class="nav-text">[Deep Learning] 浅层神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#浅层神经网络结构"><span class="nav-number">1.1.</span> <span class="nav-text">浅层神经网络结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#单样本前向传播"><span class="nav-number">1.1.1.</span> <span class="nav-text">单样本前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前向传播向量化"><span class="nav-number">1.1.2.</span> <span class="nav-text">前向传播向量化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多样本前向传播"><span class="nav-number">1.1.3.</span> <span class="nav-text">多样本前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多样本前向传播向量化"><span class="nav-number">1.1.4.</span> <span class="nav-text">多样本前向传播向量化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#激活函数"><span class="nav-number">1.2.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sigmoid函数"><span class="nav-number">1.2.1.</span> <span class="nav-text">sigmoid函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tanh函数"><span class="nav-number">1.2.2.</span> <span class="nav-text">tanh函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ReLU函数（线性修正单元）"><span class="nav-number">1.2.3.</span> <span class="nav-text">ReLU函数（线性修正单元）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Leaky-ReLU（带泄露的ReLU）"><span class="nav-number">1.2.4.</span> <span class="nav-text">Leaky ReLU（带泄露的ReLU）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#激活函数求导"><span class="nav-number">1.3.</span> <span class="nav-text">激活函数求导</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sigmoid"><span class="nav-number">1.3.1.</span> <span class="nav-text">sigmoid</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tanh-函数"><span class="nav-number">1.3.2.</span> <span class="nav-text">tanh 函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ReLU函数"><span class="nav-number">1.3.3.</span> <span class="nav-text">ReLU函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络中的梯度下降"><span class="nav-number">1.4.</span> <span class="nav-text">神经网络中的梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-number">1.4.1.</span> <span class="nav-text">反向传播</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#随机初始化"><span class="nav-number">1.5.</span> <span class="nav-text">随机初始化</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Miracle" src="/images/my.jpg"><p class="site-author-name" itemprop="name">Miracle</p><div class="site-description" itemprop="description">也无风雨也无晴</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">5</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">4</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">5</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/Mirac1eSky" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Mirac1eSky" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="external nofollow noopener noreferrer" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Miracle</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">25k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">1:01</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:inline-flex"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:inline-flex"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script>CONFIG.page.isPost&&(wpac_init=window.wpac_init||[],wpac_init.push({widget:"Rating",id:24411,el:"wpac-rating",color:"fc6423"}),function(){if(!("WIDGETPACK_LOADED"in window)){WIDGETPACK_LOADED=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//embed.widgetpack.com/widget.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t.nextSibling)}}())</script><script src="/js/local-search.js"></script><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/hijiki.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0},react:{opacity:.7}})</script></body></html>