<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/M-apple.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/M-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/M-16x16.png"><link rel="mask-icon" href="/images/M.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"miraclesky.cn",root:"/",scheme:"Muse",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!0},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="【吴恩达深度学习系列第二课第一周笔记】现在开始记录第二课程的笔记。会慢慢更新的。 关键概念：  不同类型的初始化会导致不同的结果  认识复杂神经网络初始化的重要性  识别 train &#x2F; dev &#x2F; test 集之间的差异  诊断模型中的偏差和方差问题  学习何时以及如何使用规范化方法  理解深度学习中的实验问题  使用梯度检查来验证反向传播实现的正确性"><meta property="og:type" content="article"><meta property="og:title" content="吴恩达深度学习系列第二课第一周笔记"><meta property="og:url" content="https://miraclesky.cn/posts/1ca99e0.html"><meta property="og:site_name" content="醉后凉风起，吹人舞袖回"><meta property="og:description" content="【吴恩达深度学习系列第二课第一周笔记】现在开始记录第二课程的笔记。会慢慢更新的。 关键概念：  不同类型的初始化会导致不同的结果  认识复杂神经网络初始化的重要性  识别 train &#x2F; dev &#x2F; test 集之间的差异  诊断模型中的偏差和方差问题  学习何时以及如何使用规范化方法  理解深度学习中的实验问题  使用梯度检查来验证反向传播实现的正确性"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://i.loli.net/2020/04/16/s7D4uXlU5OikgQB.png"><meta property="og:image" content="https://i.loli.net/2020/04/16/dyigBtOIGLu3nzc.png"><meta property="og:image" content="https://i.loli.net/2020/04/25/l2HVWaIUoC9Q76A.png"><meta property="og:image" content="https://i.loli.net/2020/04/25/De31642s5GbaMJv.png"><meta property="article:published_time" content="2020-04-16T07:30:36.000Z"><meta property="article:modified_time" content="2020-04-16T07:30:36.000Z"><meta property="article:author" content="Miracle"><meta property="article:tag" content="AI"><meta property="article:tag" content="人工智能"><meta property="article:tag" content="吴恩达深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://i.loli.net/2020/04/16/s7D4uXlU5OikgQB.png"><link rel="canonical" href="https://miraclesky.cn/posts/1ca99e0.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>吴恩达深度学习系列第二课第一周笔记 | 醉后凉风起，吹人舞袖回</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="醉后凉风起，吹人舞袖回" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">醉后凉风起，吹人舞袖回</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">唯见月寒日暖</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i> 公益 404</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div> <a href="https://github.com/Mirac1eSky" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="external nofollow noopener noreferrer" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"/></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://miraclesky.cn/posts/1ca99e0.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/my.jpg"><meta itemprop="name" content="Miracle"><meta itemprop="description" content="也无风雨也无晴"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="醉后凉风起，吹人舞袖回"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 吴恩达深度学习系列第二课第一周笔记</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-04-16 15:30:36" itemprop="dateCreated datePublished" datetime="2020-04-16T15:30:36+08:00">2020-04-16</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:inline-flex"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>3.9k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>10 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><script type="text/javascript" src="/js/baidu.js"></script><script type="text/javascript" src="/js/360.js"></script><h1 id="【吴恩达深度学习系列第二课第一周笔记】"><a href="#【吴恩达深度学习系列第二课第一周笔记】" class="headerlink" title="【吴恩达深度学习系列第二课第一周笔记】"></a>【吴恩达深度学习系列第二课第一周笔记】</h1><p>现在开始记录第二课程的笔记。会慢慢更新的。</p><p><strong>关键概念</strong>：</p><ul><li><p>不同类型的初始化会导致不同的结果</p></li><li><p>认识复杂神经网络初始化的重要性</p></li><li><p>识别 train / dev / test 集之间的差异</p></li><li><p>诊断模型中的偏差和方差问题</p></li><li><p>学习何时以及如何使用规范化方法</p></li><li><p>理解深度学习中的实验问题</p></li><li><p>使用梯度检查来验证反向传播实现的正确性</p></li></ul><a id="more"></a><h2 id="正确设置训练集，验证集（开发集）测试集"><a href="#正确设置训练集，验证集（开发集）测试集" class="headerlink" title="正确设置训练集，验证集（开发集）测试集"></a>正确设置训练集，验证集（开发集）测试集</h2><p>在机器学习小数据量时代即当你的数据集在100,1000,10000的时候，可以采用60%：20%：20%的比例来，即用60%的数据来训练模型，20%的数据来评估不同你不同的模型，最后用20%的数据集来评估选定模型的性能。</p><p>但是当数量达到百万级别时，我们就可能不在需要这么高的比例来划分出验证集。因为验证集的目的就是用来快速验证不同的算法之间的差别，所以可能不再不要这么多的数据作为验证集。举个例子，当我们有一百万的数据时，可能只需要一万条数据来验证就行了。同样的，最后的测试集也是如此。所以最终的比例就是98%：1%：1%。对于数据量过百万的情况，可能就是99.5%：0.25%：0.25%.</p><h2 id="训练集和测试集分布不匹配"><a href="#训练集和测试集分布不匹配" class="headerlink" title="训练集和测试集分布不匹配"></a>训练集和测试集分布不匹配</h2><p>比如说你有个应用，通过用户上传图片，然后告诉用户这张图片是不是猫咪。相比较而言，你的训练集都是从网上下载下来的高分辨率图片，并且构图精巧。而用户上传的图片可能分辨率就不是很高，并且更加随意。这个时候你的训练集和测试集的分布就不一样了。对此，吴老师的建议是确保分布相同，具体会在后面的课程提到。</p><h2 id="偏差和方差以及偏差方差权衡"><a href="#偏差和方差以及偏差方差权衡" class="headerlink" title="偏差和方差以及偏差方差权衡"></a>偏差和方差以及偏差方差权衡</h2><p>对于偏差和方差问题应该都会有大致了解了。比如高偏差对应欠拟合，高方差对应了过拟合。如图所示：</p><p><img src="https://i.loli.net/2020/04/16/s7D4uXlU5OikgQB.png" style="zoom:50%"></p><p>接下来看看如何分析偏差和方差：</p><p>假设你有个识别猫咪的分类器，得到了不同的误差集合：</p><p><img src="https://i.loli.net/2020/04/16/dyigBtOIGLu3nzc.png" alt></p><ol><li>当训练集误差是第一列时，可以看到在训练集上误差很小，但是在验证集上误差很大，说明过拟合了。</li><li>当训练集为第二列时，可以看到在训练集和验证集上的表现都不是很好，说明该模型在训练集是欠拟合，而在验证集上表现也符合该预测</li><li>当训练集为第三列时，训练集和验证集的表现都不行，并且验证集的表现比训练集差很多。说明该模型不仅存在高偏差的问题，并且还存在高方差的问题。也就是说该分类器在总体上是欠拟合的，但是在部分数据上又表现出过拟合的问题。</li><li>第四列则是开发人员很希望看到的，就是在训练集和验证集上的误差都很小，说明是适拟合的情况。</li></ol><h2 id="机器学习的基本方法"><a href="#机器学习的基本方法" class="headerlink" title="机器学习的基本方法"></a>机器学习的基本方法</h2><p>当训练好你的网络，首先应该看看是否存在高偏差的问题：</p><ul><li>如果存在，则尝试减少，例如使用更大的网络，训练的更久一点等等这些方法</li><li>如果不存在，则看看有没有高方差的问题。如果存在，尝试获得更多的数据，或者正则化</li></ul><p>尝试上面的方法，直到找到低偏差，低方差的网络。</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="正则化的定义"><a href="#正则化的定义" class="headerlink" title="正则化的定义"></a>正则化的定义</h3><p>如果你的NN发生了高偏差，一般可以采用正则化来尝试解决问题。获取更多数据也是解决高方差问题的一个很可靠的方法 ，但你并不是总能获取到更多的训练数据 ，或者获取更多数据的代价太大 。但使用正则化通常有助于防止过拟合 并降低网络的误差。</p><h4 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h4><p>在逻辑回归中，你会尝试最小化代价函数<strong>J</strong> ，该代价函数定义为 ：每一个训练样本的预测的损失之和 其中w和b是 逻辑回归的参数 ，因此w是一个x维的参数向量 b是一个实数，要为逻辑回归正则化 你需要加上这个lambda 它称为正则化参数。正则项被定义为：</p><script type="math/tex;mode=display">
\frac{\lambda}{2m}||w||^{2}_2</script><script type="math/tex;mode=display">
||w||^{2}_2 =\sum_{j=1}^{n_x}w_j^2 = w^Tw</script><p>这里的<script type="math/tex">||w||^{2}_2</script>也可以叫做w的范数，也被称作L2正则化。与此同时，有另一种正则化叫做L1正则化，如下：</p><script type="math/tex;mode=display">
\sum_{j=1}^{n_x}|w_j| =||w||_1</script><p>如果使用L1正则化，w最后会变得稀疏，这意味着w矢量中有很多0 。有些人认为这有助于压缩模型 ，因为有一部分参数是0 只需较少的内存来存储模型，然而在实践中发现 通过L1正则化让模型变得稀疏，带来的收效甚微 所以我觉得至少在压缩模型的目标上 它的作用不大 。在训练网络时，L2正则化使用得频繁得多。（注意：==在python中，lambda是保留关键字，所以用lambd代替==）。在上面发现，正则化参数时，并没有将b包含进来，实际上你可以这样做，但通常会把它省略掉。因为你可以看一下你的参数w往往是一个非常高维的参数矢量，尤其是在发生高方差问题的情况下 可能w有非常多的参数，你没能很好地拟合所有的参数，而b只是单个数字，几乎所有的参数都集中在w中 而不是b中，即使你加上了最后这一项，实际上也不会起到太大的作用。因为b只是大量参数中的一个参数。</p><h4 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h4><p>在神经网络中，你的正则项定义如下：</p><script type="math/tex;mode=display">
\frac{\lambda}{2m}||W^{[l]}||^{2}_F</script><script type="math/tex;mode=display">
||W^{[l]}||^{2} = \sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}(W_{ij}^{[l]})^2</script><p>这个矩阵的范数称为矩阵的弗罗贝尼乌斯范数，使用角标F标记。由于线性代数中某些神秘的技术原因这不叫矩阵的L2范数而是称为矩阵的弗罗贝尼乌斯范数，它表示矩阵中元素的平方和。</p><h3 id="为什么正则化可以解决过拟合"><a href="#为什么正则化可以解决过拟合" class="headerlink" title="为什么正则化可以解决过拟合"></a>为什么正则化可以解决过拟合</h3><p>看个例子</p><p><img src="https://i.loli.net/2020/04/25/l2HVWaIUoC9Q76A.png" style="zoom:50%"></p><p>在高方差的情况下，当你把lambda的值设置的很大，就相当于把W设置为非常接近0的值。也就是说隐藏层的影响会很小，因此这个网络最终就相当于逻辑回归模型。但是lambda存在一个中间值，可以把高偏差这种情况变成”just right”。</p><h3 id="另一种常见正则化——随机失活正则化-丢弃法-dropout"><a href="#另一种常见正则化——随机失活正则化-丢弃法-dropout" class="headerlink" title="另一种常见正则化——随机失活正则化(丢弃法 dropout)"></a>另一种常见正则化——随机失活正则化(丢弃法 dropout)</h3><p>这种方法就是在你的网络上，遍历每个节点。然后以一定的概率丢弃节点，最后形成一个小的多的网络，在这上面进行反向传播</p><p><img src="https://i.loli.net/2020/04/25/De31642s5GbaMJv.png" style="zoom:80%"></p><p>下面看个栗子，如何用代码实现丢弃法，（反向随机失活）</p><p>首先定义一个阈值keep.prob = 0.8,然后随机生成一个新的矩阵 d3 = np.random.rand(a3.shape[0],a3.shape[1]) &lt; keep.prob。在这个矩阵中每个元素有80%的概率为1,20%的概率为0（严格来说，这是一个True和False的矩阵，但是在python中这样是可行的）。 a3 = np.multiply(a3,d3)或者 a3 *= d3。这样就可以实现随机丢弃了。但是到这还没结束，需要以下这一步骤 a3 /= keep.prob。来解释一下它。假设a3层有50个隐藏单元，所以a3的维数为50x1，如果是矢量化运算则为50 x m。每个单元有80%的概率保留，20%的概率丢弃，这意味着平均下来有10个单元被丢弃。而<script type="math/tex">z^{[4]} = w^{[4]}a^{[3]}+b^{[4]}</script> 。所以<script type="math/tex">a^{[3]}</script>的期望值会减少20%,e为了不让<script type="math/tex">z^{[4]}</script>的期望不被减少，就让<script type="math/tex">a^{[3]}</script>除以0.8，这样就可以修正期望值。这就是反向随机失活。需要注意的是，在每次迭代训练中，每次丢弃的都应该不同，而不是一直丢弃相同的单元。</p><h4 id="理解随机失活"><a href="#理解随机失活" class="headerlink" title="理解随机失活"></a>理解随机失活</h4><p>使用随机失活后，好像使用一个小的网络，就可以实现正则化。那么是为什么是这样呢？因为对于某个隐藏单元来说，它的每个输入都可能被丢弃，所以会尽可能的分散权重，不让某个输入的权重过大。这样就有了收缩权重以及防止过拟合的功能。准确的来讲，随机失活应该被看做一种自适应而不是正则化。</p><p>在网络上使用这张方法时，不同的隐藏层可以使用 不同的阈值，比如某些层你认为不会过拟合，则将阈值设为1,。但是这么做的代价是超参数过多导致运行较慢。另一种方法是在某些隐藏层设定相同的阈值，然后其它层不设置。</p><h3 id="其他正则化技术"><a href="#其他正则化技术" class="headerlink" title="其他正则化技术"></a>其他正则化技术</h3><h4 id="数据集增广-data-augmentation"><a href="#数据集增广-data-augmentation" class="headerlink" title="数据集增广(data augmentation)"></a>数据集增广(data augmentation)</h4><p>我们知道，增加训练集，可以解决过拟合问题。所以在现有数据集的情况下，我们可以对图片进行一些处理，；来新增数据，比如对图片进行旋转，放大，裁剪等等。这样我们会得到新的数据集。这也是解决过拟合的一种手段。</p><h4 id="早终止法-early-stopping"><a href="#早终止法-early-stopping" class="headerlink" title="早终止法(early stopping)"></a>早终止法(early stopping)</h4><p>这种方法就是画出在训练集和开发集上代价函数关于迭代次数的图像。在开发集的代价函数最小时提前终止训练。</p><h2 id="优化网络"><a href="#优化网络" class="headerlink" title="优化网络"></a>优化网络</h2><h3 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h3><p>首先如何归一化呢？先算出输入x的平均值，再全体减去均值。然后令σ^2等于 Xi**2之和除以m ，最后在同除以σ^2。同时也要对测试集，开发集进行归一化。</p><h4 id="为什么要归一化"><a href="#为什么要归一化" class="headerlink" title="为什么要归一化"></a>为什么要归一化</h4><p>因为不用的特征的范围是不同的，有的范围从0-1，有的0-100，这就会导致权重比例不平衡，并且会导致梯度下降很慢。当归一化之后，数据范围都是相近的，就不会出现上述情况了。</p><h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p>当训练神经网络时我们会遇到一个问题，尤其是当训练层数非常多的神经网络时。这个问题就是梯度的消失和爆炸 它的意思是当你在训练一个深度神经网络的时候，损失函数的导数或者说斜率有时会变得非常大或者非常小甚至是呈指数级减小，这使训练变得很困难。举个栗子，你的激活函数为线性函数，当你的网络非常深，并且权重矩阵大于1的时候这个时候就会出现梯度爆炸的问题，事实上的最后的预测值是指数级增长的。同样，当权重矩阵的值小于1时，就会出现梯度消失的问题。</p><h4 id="如何尽量避免梯度消失和梯度爆炸"><a href="#如何尽量避免梯度消失和梯度爆炸" class="headerlink" title="如何尽量避免梯度消失和梯度爆炸"></a>如何尽量避免梯度消失和梯度爆炸</h4><ul><li>随机初始化权重矩阵</li></ul><p>假设你的激活函数为ReLu，那么可以这样初始化：</p><script type="math/tex;mode=display">
W^{[l]} = np.random.randn(shape) * np.sqrt(\frac{2}{n^{[l-1]}})</script><p>如果是tanh：</p><script type="math/tex;mode=display">
W^{[l]} = np.random.randn(shape) * np.sqrt(\frac{1}{n^{[l-1]}})</script><p>或者</p><script type="math/tex;mode=display">
W^{[l]} = np.random.randn(shape) * np.sqrt(\frac{2}{n^{[l-1]}+n^{[l]}})</script></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">------------------已经触及底线啦<i class="fa fa-paw"></i>感谢您的阅读------------------</div></div></div><div class="reward-container"><div></div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechat.jpg" alt="Miracle 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="Miracle 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> Miracle</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://miraclesky.cn/posts/1ca99e0.html" title="吴恩达深度学习系列第二课第一周笔记">https://miraclesky.cn/posts/1ca99e0.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="/atom.xml"><span class="icon"><i class="fa fa-rss"></i></span> <span class="label">RSS</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%96%B9%E5%B7%AE%E5%81%8F%E5%B7%AE/" rel="tag"><i class="fa fa-tag"></i> 方差偏差</a></div><div class="post-widgets"><div class="wp_rating"><div id="wpac-rating"></div></div></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/206c9e3b.html" rel="prev" title="力扣每日一题"><i class="fa fa-chevron-left"></i> 力扣每日一题</a></div><div class="post-nav-item"> <a href="/posts/7c892205.html" rel="next" title="python安装PyUserInput的问题">python安装PyUserInput的问题<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC80OTUwOS8yNjAwMA"></div></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#【吴恩达深度学习系列第二课第一周笔记】"><span class="nav-number">1.</span> <span class="nav-text">【吴恩达深度学习系列第二课第一周笔记】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#正确设置训练集，验证集（开发集）测试集"><span class="nav-number">1.1.</span> <span class="nav-text">正确设置训练集，验证集（开发集）测试集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练集和测试集分布不匹配"><span class="nav-number">1.2.</span> <span class="nav-text">训练集和测试集分布不匹配</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#偏差和方差以及偏差方差权衡"><span class="nav-number">1.3.</span> <span class="nav-text">偏差和方差以及偏差方差权衡</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习的基本方法"><span class="nav-number">1.4.</span> <span class="nav-text">机器学习的基本方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化"><span class="nav-number">1.5.</span> <span class="nav-text">正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#正则化的定义"><span class="nav-number">1.5.1.</span> <span class="nav-text">正则化的定义</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归模型"><span class="nav-number">1.5.1.1.</span> <span class="nav-text">逻辑回归模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#神经网络"><span class="nav-number">1.5.1.2.</span> <span class="nav-text">神经网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么正则化可以解决过拟合"><span class="nav-number">1.5.2.</span> <span class="nav-text">为什么正则化可以解决过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#另一种常见正则化——随机失活正则化-丢弃法-dropout"><span class="nav-number">1.5.3.</span> <span class="nav-text">另一种常见正则化——随机失活正则化(丢弃法 dropout)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#理解随机失活"><span class="nav-number">1.5.3.1.</span> <span class="nav-text">理解随机失活</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其他正则化技术"><span class="nav-number">1.5.4.</span> <span class="nav-text">其他正则化技术</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据集增广-data-augmentation"><span class="nav-number">1.5.4.1.</span> <span class="nav-text">数据集增广(data augmentation)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#早终止法-early-stopping"><span class="nav-number">1.5.4.2.</span> <span class="nav-text">早终止法(early stopping)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化网络"><span class="nav-number">1.6.</span> <span class="nav-text">优化网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#归一化"><span class="nav-number">1.6.1.</span> <span class="nav-text">归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么要归一化"><span class="nav-number">1.6.1.1.</span> <span class="nav-text">为什么要归一化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度消失和梯度爆炸"><span class="nav-number">1.6.2.</span> <span class="nav-text">梯度消失和梯度爆炸</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#如何尽量避免梯度消失和梯度爆炸"><span class="nav-number">1.6.2.1.</span> <span class="nav-text">如何尽量避免梯度消失和梯度爆炸</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Miracle" src="/images/my.jpg"><p class="site-author-name" itemprop="name">Miracle</p><div class="site-description" itemprop="description">也无风雨也无晴</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">10</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">7</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/Mirac1eSky" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Mirac1eSky" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="external nofollow noopener noreferrer" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Miracle</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">64k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">2:39</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:inline-flex"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:inline-flex"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script>CONFIG.page.isPost&&(wpac_init=window.wpac_init||[],wpac_init.push({widget:"Rating",id:24411,el:"wpac-rating",color:"fc6423"}),function(){if(!("WIDGETPACK_LOADED"in window)){WIDGETPACK_LOADED=!0;var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//embed.widgetpack.com/widget.js";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t.nextSibling)}}())</script><script src="/js/local-search.js"></script><script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({pluginRootPath:"live2dw/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,log:!1,model:{jsonPath:"/live2dw/assets/hijiki.model.json"},display:{position:"left",width:150,height:300},mobile:{show:!0},react:{opacity:.7}})</script></body></html>