<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>NexT升级过程中遇到的坑</title>
      <link href="/posts/cbb37015.html"/>
      <url>/posts/cbb37015.html</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="/js/baidu.js"></script><h1 id="NexT从5-1-x-—-gt-7-8-0-中遇到的坑"><a href="#NexT从5-1-x-—-gt-7-8-0-中遇到的坑" class="headerlink" title="NexT从5.1.x  —&gt; 7.8.0 中遇到的坑"></a>NexT从5.1.x  —&gt; 7.8.0 中遇到的坑</h1><blockquote><p>今天闲来无事，发现NexT已经更新到7.8.0，而自己前几天刚刚搭好的博客还是5.1版本，内心十万只羊驼呼啸而过。。。虽然很不情愿，但本着早生晚生都要生的态度，就决定在折腾一把，进行升级。。。谨以此文，纪念我升级过程中遇到的大坑。</p></blockquote><a id="more"></a><p>升级过程其实很简单，直接从github上clone下来就行了，<a href="git clone https://github.com/theme-next/hexo-theme-next themes/">具体可以参考官方文档</a>，这里不过多阐述。直接说遇到的坑吧！</p><ul><li><h2 id="显示的字体变成梵文？？"><a href="#显示的字体变成梵文？？" class="headerlink" title="显示的字体变成梵文？？"></a>显示的字体变成梵文？？</h2><p>其实这个问题是因为没有好好看官方文档导致的。。在5.1版本中，中文设置是叫zh-Hans，在新版本中，中文应该设置为 zh-CN 。之后之间刷新就好了，如果没有效果，先clean一下，之后正常显示中文了。</p></li></ul><ul><li><h2 id="给文章添加的阴影效果不见了"><a href="#给文章添加的阴影效果不见了" class="headerlink" title="给文章添加的阴影效果不见了"></a>给文章添加的阴影效果不见了</h2></li></ul><p>在5.1版本中，可以通过修改_custom下的custom.styl来覆盖原有样式，但是在新版本中，目录结构发生了变换，导致这个方法失效了，所以只能通过其他手段实现。然后在<strong>主题配置</strong>文件中，可以找到以下配置：</p><p><img src="https://i.loli.net/2020/04/07/HnJgxFha7VvdBMj.png" alt></p><p>我们将红框圈出来的这一行的注释去掉，然后在 <strong>站点根目录</strong>下的source目录下，新建_data目录，再新建styles.styl。走到这一步，原本以为成功了，将之前代码复制进去，发现还是没有用。。。后来发现是因为样式名已经变了，所以需要重写，下面是添加的代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">.use-motion &#123;</span><br><span class="line">if (hexo-config(&#39;motion.transition.post_block&#39;)) &#123;</span><br><span class="line">.post-block &#123;</span><br><span class="line">   opacity: 0;</span><br><span class="line">   margin-top: 60px;</span><br><span class="line">   margin-bottom: 60px;</span><br><span class="line">   padding: 25px;</span><br><span class="line">   -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5);</span><br><span class="line">   -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);</span><br><span class="line">&#125;</span><br><span class="line">.pagination, .comments &#123;</span><br><span class="line">opacity: 0;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后在执行hexo clean &amp; hexo s ，就可以在本地看到消失的阴影框又出现了。</p><ul><li><h2 id="文章置顶的功能"><a href="#文章置顶的功能" class="headerlink" title="文章置顶的功能"></a>文章置顶的功能</h2></li></ul><p>这个功能其实还好，和5.1版本一样，直接修改 themes*\layout\_macro目录下的post.swig</p><p>定位到 div class=”post-meta” 标签下，插入如下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if post.top %&#125;</span><br><span class="line">    &lt;i class&#x3D;&quot;fa fa-thumb-tack&quot;&gt;&lt;&#x2F;i&gt;</span><br><span class="line">    &lt;font color&#x3D;green&gt;置顶&lt;&#x2F;font&gt;</span><br><span class="line">    &lt;span class&#x3D;&quot;post-meta-divider&quot;&gt;|&lt;&#x2F;span&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>然后保存刷新，就可以看到效果了。（首先要安装hexo-generator-index-pin-top插件，以及卸载掉原本的hexo-generator-index插件）</p><ul><li><h2 id="添加文章阅读标记"><a href="#添加文章阅读标记" class="headerlink" title="添加文章阅读标记"></a>添加文章阅读标记</h2></li></ul><p>这个功能其实也和老版本一样，没有改动。在<code>\themes\*\layout\_macro</code>中新建<code>passage-end-tag.swig</code>文件，然后添加以下代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">    &#123;% if not is_index %&#125;</span><br><span class="line">        &lt;div style&#x3D;&quot;text-align:center;color: #ccc;font-size:14px;&quot;&gt;-------------已经触及底线啦&lt;i class&#x3D;&quot;fa fa-paw&quot;&gt;&lt;&#x2F;i&gt;感谢您的阅读-------------&lt;&#x2F;div&gt;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line">&lt;&#x2F;div&gt;</span><br></pre></td></tr></table></figure><p>接着在统计目录下，修改post.swig,在<code>post-body</code>后，<code>END POST BODY</code>前，添加以下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">  &#123;% if not is_index %&#125;</span><br><span class="line">    &#123;% include &#39;passage-end-tag.swig&#39; %&#125;</span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line">&lt;&#x2F;div&gt;</span><br></pre></td></tr></table></figure><p>最后在<strong>主题配置文件</strong>中 ，添加以下配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">passage_end_tag:</span><br><span class="line">  enabled: true</span><br></pre></td></tr></table></figure><p>这样可以实现阅读结束标记了==</p><ul><li><h2 id="不蒜子-不显示"><a href="#不蒜子-不显示" class="headerlink" title="不蒜子 不显示"></a>不蒜子 不显示</h2><p>在更新的7.8版本中，有关于不蒜子的设置，将其设置为true后，还是不显示访问人数，在baidu后发现都是说调用api出错，但是经过查看后发现，api并没有错。。。所以在打开开发者模式后才发现，不蒜子的数据是有的，但是样式后面加上了display:none。。。所以该样式并没有显示。然后在themes\your_theme\layout_third-party\statistics目录下打开busuanzi-counter.swig以及themes\your_theme\layout_macro目录下的post.swig,果然发现了该样式，如图:</p><p><img src="https://i.loli.net/2020/04/07/rSPbIfQn1gl3iUC.png"></p></li></ul><p>  <img src="https://i.loli.net/2020/04/07/rv6Xut9LGczRyqY.png" alt></p><p>  然后直接将样式修改为”display: inline-flex;”,重新启动，果然主页数据出来的，但是点进去具体文章页面，还是没有访问统计。。。实在无语，索性在之前添加的自定义样式文件styles.styl中，添加以下代码，强制覆盖CSS样式：</p><p>  <img src="https://i.loli.net/2020/04/07/LIqgQOTmkafZwRY.png" alt></p><p>  然后执行hexo clean &amp; hexo s ，清除一下缓存，就可以看到数据显示咯~终于成功了，属实坑爹。。</p><p>  效果如下图：</p><p>  <img src="https://i.loli.net/2020/04/07/gAcD6nksmPJd4Ty.png" alt></p><p>  <img src="https://i.loli.net/2020/04/07/XLHCYn6mjWx7VE2.png" alt></p><p>  <img src="https://i.loli.net/2020/04/07/hXeKVJCOzQRZWod.png" alt></p><hr><p>目前就遇到这些问题，如果遇到新问题，还是会追加更新的~</p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
          <category> next </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 版本升级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>neural-networks-deep-learning-week4</title>
      <link href="/posts/d78210a7.html"/>
      <url>/posts/d78210a7.html</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="/js/baidu.js"></script><h1 id="Deep-Learning-深层神经网络"><a href="#Deep-Learning-深层神经网络" class="headerlink" title="[Deep Learning] 深层神经网络"></a>[Deep Learning] 深层神经网络</h1><p>本周目标：</p><ul><li><p>将深层神经网络视为一个接一个的连续块</p></li><li><p>建立和训练深层L层神经网络</p></li><li><p>分析矩阵和向量维数以检查神经网络的实现。</p></li><li><p>了解如何使用缓存将信息从正向传播传递到反向传播。</p></li><li><p>了解超参数在深度学习中的作用</p><a id="more"></a></li></ul><hr><h1 id="块的思想"><a href="#块的思想" class="headerlink" title="块的思想"></a>块的思想</h1><ul><li>对于每层的计算，可以将其视为一个块，如下图</li></ul><p>  <img src="https://i.loli.net/2020/04/06/I8hkFCdOqEYH1bm.jpg" alt></p><p>  对于此图，只需要理解其过程即可，在前向传播过程中，将变量Z1,Z2等等存储起来，然后在反向传播过程中，就可以很方便地使用他们。</p><h1 id="检查矩阵维度"><a href="#检查矩阵维度" class="headerlink" title="检查矩阵维度"></a>检查矩阵维度</h1><ul><li><p>在写代码过程中，检查各个参数的维度，能有效的避免一些奇怪的BUG。用下图举个栗子</p><p><img src="https://i.loli.net/2020/04/06/74IfwvqOrUk5Wb2.png" style="zoom:50%;"></p><p>如图所示，这是一个五层的神经网络，有参数<script type="math/tex">W^{[1]},b^{[1]}...</script>  首先，先确定<script type="math/tex">W^{[1]}</script>的维度，可以看到，第一个隐藏层有三个单元，即 <script type="math/tex">n^{[1]} = 3</script> ,而输入为<script type="math/tex">x_1,x_2</script>，即<script type="math/tex">n^{[0]}=n_x=2</script>,可以看做是一个 <strong>2 x 1</strong>的列向量。又有<script type="math/tex">z^{[1]} = W^{[1]}x + b^{[1]}</script>,先忽略<script type="math/tex">b^{[1]}</script>，我们已经知道<script type="math/tex">z^{[1]}</script>是 <strong>3 x 1</strong> 的列向量，那么根据线性代数的有关知识，可以直接推断出<script type="math/tex">W^{[1]}</script>是 <strong>3 x 2</strong> 的矩阵。<strong>（若有 A矩阵 的维度为 m x k ,B矩阵的维度为 k x n,那么 AB矩阵相乘得到的矩阵C的维度为 m x n）</strong>。那<script type="math/tex">b^{[1]}</script>的矩阵维度为 <strong>3 x 1</strong>。同理，我们可以推断出 <script type="math/tex">W^{[2]}</script>的维度为 <strong>4 x 3</strong>，<script type="math/tex">b^{[2]}</script> 的维度为 <strong>4 x 1</strong> …..</p><p>由此，可以归纳出一个维度公式，即对于第k层中<strong>(k&gt;0）</strong>，有</p><script type="math/tex; mode=display">\begin{array}\\W^{[k]}的维度 = n^{[k]} * n^{[k-1]} \\b^{[k]}的维度 = n^{[k]} * 1\end{array}</script><p>得到每个参数的维度后，就可以很方便的检验运算是否正确咯。</p></li></ul><h1 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h1><ul><li><h2 id="什么是超参数"><a href="#什么是超参数" class="headerlink" title="什么是超参数"></a>什么是超参数</h2><ul><li>在整个NN中，有许多参数，比如<script type="math/tex">W^{[1]},b^{[1]},W^{[2]},b^{[2]}</script>等等，还有一些其他参数，比如<script type="math/tex">学习率\alpha，迭代次数\#iterations,隐藏层数L，隐藏单元个数n^{[l]},激活函数等等</script>。 而这些这参数都会影响到最终的W,b的结果，所以这些参数就被称作超参数。实际上还有很多超参数，具体的会在之后详细谈及。</li></ul></li><li><h2 id="超参数的作用"><a href="#超参数的作用" class="headerlink" title="超参数的作用"></a>超参数的作用</h2><ul><li><p>在深度学习算法中的超参数如何取值是一个以实验为依据的过程，有时候，可能依赖直觉，比如设置<script type="math/tex">\alpha = 0.01</script>,然后实际操作了一下，得到了某个结果，但是对这个结果不满意，于是把<script type="math/tex">\alpha</script>的值增加到0.05。</p><p>大部分情况下，都很难提前知道这些超参数的最优解，所以具体的取值其实是一个基于试验的过程，在过程中发现新的最优解。</p></li></ul></li></ul><h1 id="多层NN反向传播"><a href="#多层NN反向传播" class="headerlink" title="多层NN反向传播"></a>多层NN反向传播</h1><ul><li><p>在这里，我们具体讨论一下，关于多层网络的反向传播算法。对于前向传播，相信大家应该都很熟悉了，这列就不过多陈述了。主要还是反向传播（Back Propagation，简称BP）。在BP中，第一层的输入应该是在<script type="math/tex">da^{[l]}</script>,而输出是<script type="math/tex">dW^{[l]},db^{[l]}</script>。 </p><p>假设一共有L层网络，Y为真实的label，则在第L层，有</p><script type="math/tex; mode=display">\begin{array}\\dZ^{[L]} = A^{[L]} - Y \\dW^{[L]} = \left(\frac{1}{m}\right) dZ^{[L]}  A^{[L-1]T} \\db^{[L]} = \left(\frac{1}{m}\right)dZ^{[L]}\end{array}</script></li></ul><p>  对于接下来的L-1层，有</p><script type="math/tex; mode=display">  \begin{array}  \\  dZ^{[L-1]} = W^{[L]T} dZ^{[L]} * g^{[L]'}(Z^{[L-1]})  \\  dW^{[l-1]} = \left(\frac{1}{m}\right) dZ^{[L-1]} * A^{[L-2]T} \\  db^{[L-1]} = \left(\frac{1}{m}\right)dZ^{[L-1]}  \end{array}</script><p>  这里公式是针对<font color="gree">所有样本</font>而言，且认为是做二元分类，即最后的<font color="gree">激活函数为sigmoid</font>。 <font color="gree">*</font>  代表 对应元素相乘，即A矩阵为 m x n, B矩阵为 m x n ，那么A * B 依旧是 m x n的矩阵。有了对应的梯度，我们就可以利用for循环来实现对L层网络的反向传播，这里要注意第一次梯度下降时要分开。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>neural-networks-deep-learning-week3</title>
      <link href="/posts/49e68504.html"/>
      <url>/posts/49e68504.html</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="/js/baidu.js"></script><h1 id="Deep-Learning-浅层神经网络"><a href="#Deep-Learning-浅层神经网络" class="headerlink" title="[Deep Learning] 浅层神经网络"></a>[Deep Learning] 浅层神经网络</h1><ul><li>理解隐藏层和隐藏单元</li><li>能够使用多种类型的激活函数</li><li>用隐藏层建立一个前向传播和反向传播算法</li><li>应用随机初始化</li></ul><a id="more"></a><h2 id="浅层神经网络结构"><a href="#浅层神经网络结构" class="headerlink" title="浅层神经网络结构"></a>浅层神经网络结构</h2><ul><li><h3 id="单样本前向传播"><a href="#单样本前向传播" class="headerlink" title="单样本前向传播"></a>单样本前向传播</h3><p>首先，看一下神经网络的结构图 </p><p><img src="https://i.loli.net/2020/04/06/lR8p6ri9qhuVMcf.png" alt="NNMODEL.png"></p></li></ul><p>这是一个两层的神经网络（输入层不算做一层，认为是第0层）。一般用 <strong>X</strong> 代表输入，也可以用 <script type="math/tex">a^{[0]}</script> 表示 （a代  表激活的意思），用<script type="math/tex">a^{[1]}</script>代表第一层即隐藏层，<script type="math/tex">a^{[2]}</script>代表输出层。所以上图中 ，<strong>X</strong> = <script type="math/tex">a^{[0]}</script> <script type="math/tex">\in R^{3}</script> ,<script type="math/tex">a^{[1]} \in R^{4}</script>,<script type="math/tex">a^{[2]}\in R</script> 。 现在来看隐藏层第一个节点即<script type="math/tex">a^{[1]}_{1}</script>的计算，这个计算和逻辑回归一样。首先算出</p><script type="math/tex; mode=display">z^{[1]}_1 = w^{[1]T}_1x + b^{[1]}_1</script><p> 然后计算</p><script type="math/tex; mode=display">a^{[1]}_1 = sigmoid(z^{[1]}_1)</script><p> 其他节点 同理计算，整理一下</p><script type="math/tex; mode=display">z^{[1]}_1 = w^{[1]T}_1x + b^{[1]}_1,a^{[1]}_1 = sigmoid(z^{[1]}_1)</script><script type="math/tex; mode=display">z^{[1]}_2 = w^{[1]T}_2x + b^{[1]}_2,a^{[1]}_2=sigmoid(z^{[1]}_1)</script><script type="math/tex; mode=display">z^{[1]}_3 = w^{[1]T}_3x + b^{[1]}_3,a^{[1]}_3=sigmoid(z^{[1]}_3)</script><script type="math/tex; mode=display">z^{[1]}_4 = w^{[1]T}_4x + b^{[1]}_4,a^{[1]}_4=sigmoid(z^{[1]}_4)</script><ul><li><h3 id="前向传播向量化"><a href="#前向传播向量化" class="headerlink" title="前向传播向量化"></a>前向传播向量化</h3><p>接下来将这几个式子向量化</p></li></ul><script type="math/tex; mode=display">\left[ \begin{matrix}   --w^{[1]T}_1-- \\   --w^{[1]T}_2-- \\   --w^{[1]T}_3-- \\   --w^{[1]T}_4--   \end{matrix}  \right] *\left[\begin{matrix}   x_1 \\   x_2 \\   x_3\end{matrix}\right]   + \left[\begin{matrix}b^{1}_1 \\b^{1}_2 \\b^{1}_3 \\b^{1}_4\end{matrix}\right]=\left[\begin{matrix}w^{[1]T}_1x + b^{[1]}_1 \\w^{[1]T}_2x + b^{[1]}_2 \\w^{[1]T}_3x + b^{[1]}_3 \\w^{[1]T}_4x + b^{[1]}_4 \end{matrix}\right]=\left[\begin{matrix} z^{[1]}_1 \\ z^{[1]}_2 \\ z^{[1]}_3 \\ z^{[1]}_4 \\\end{matrix}\right]</script><p>​      第一个矩阵为4 x 3的矩阵 ,这是因为 <script type="math/tex">w^{1}_l</script>都属于 三维列向量，现在将他们的转置竖直拼  接为新的矩阵，所以维度4x3。简化一下，可以得到下面的公式</p><script type="math/tex; mode=display">z^{[1]} = W^{[1]} x + b^{[1]}</script><script type="math/tex; mode=display">a^{[1]} = sigmoid(z^{[1]})</script><script type="math/tex; mode=display">z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}</script><script type="math/tex; mode=display">a^{[2]} = sigmoid(z^{[2]})</script><p>​      因为 x 可以写作<script type="math/tex">a^{[0]}</script>,所以</p><script type="math/tex; mode=display">z^{[1]} = W^{[1]} a^{[0]} + b^{[1]}</script><p>​      其中，<script type="math/tex">W^{[1]} \in R^{4*3},b\in R^{4*1}, W^{[2]}\in R^{1*4},b^{[2]} \in R</script>,所以最后 <script type="math/tex">a^{[2]}</script> 也是一个实数。</p><p>​     到此，就完成了一个实例的神经网络计算。</p><ul><li><h3 id="多样本前向传播"><a href="#多样本前向传播" class="headerlink" title="多样本前向传播"></a>多样本前向传播</h3></li></ul><p>  接下来 讨论一下对于多个样本的计算：</p><p>  首先解释一下符号的意思， 第<strong>i</strong>个样本第<strong>k</strong>层的节点定义为 <script type="math/tex">a^{[k](i)}</script>。</p><p>  则在计算m个样本的时候，需要像下面这样计算</p><p><strong>for i = 1 to m:</strong></p><script type="math/tex; mode=display">z^{[1](i)} = W^{[1]} x^{(i)} + b^{[1]}</script><script type="math/tex; mode=display">a^{[1](i)} = sigmoid(z^{[1](i)})</script><script type="math/tex; mode=display">z^{[2](i)} = W^{[2]}a^{[1](i)} + b^{[2]}</script><script type="math/tex; mode=display">a^{[2](i)} = sigmoid(z^{[2](i)})</script><ul><li><h3 id="多样本前向传播向量化"><a href="#多样本前向传播向量化" class="headerlink" title="多样本前向传播向量化"></a>多样本前向传播向量化</h3><p>首先将多个x样本水平拼接为<strong>X</strong>，维度为 n x m ，即</p></li></ul><script type="math/tex; mode=display">  X =   \left[  \begin{matrix}  | & |  &  | \\  x^{(1)} & x^{(2)} & x^{(3)} \\  | & |  &  | \\  \end{matrix} \tag{n * m}  \right]</script><p>  则 向量化 如下</p><script type="math/tex; mode=display">  Z^{[1]} =  W^{[1]} X + b^{[1]}</script><script type="math/tex; mode=display">A^{[1]} = sigmoid(Z^{[1]})</script><script type="math/tex; mode=display">  Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}</script><script type="math/tex; mode=display">  A^{[2]} = sigmoid(Z^{[2]})</script><p>  同样的 ，<script type="math/tex">Z^{[1]}</script> ，<script type="math/tex">A^{[1]}</script>都是矩阵。到此就已经全部完成了。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ul><li><h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p>目前为止，我们使用的激活函数一直都是sigmoid。它的图像是这样：</p></li></ul><p><img src="https://i.loli.net/2020/03/31/LlMgaBbvCmTZnyr.png" style="zoom:50%;"></p><p>可以看到，当z非常大或者非常小的时候，它斜率也非常小，这样会使梯度下降很慢。所以基本上不会使用这个激活函数。 <font color="red">除非在做二元分类的时候，需要输出值在o-1之间,只有在这个时候会在输出层选择它。需要注意的是，对于隐藏层和输出层来讲，可以选择不同的激活函数。</font></p><ul><li><h3 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h3><p>一般情况下，tanh函数都要优于sigmoid函数，它的图像如下：</p><p><img src="https://i.loli.net/2020/03/31/hzbAeauWGZRItFM.png" style="zoom:50%;"></p><p>它的公式为：</p></li></ul><script type="math/tex; mode=display">tanh = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}</script><p>其实是sigmoid函数经过变换后得来的。尽管他的表现要比sigmoid函数要好，但是仍旧面临同样的问题，就是当z过大或过小的时候，斜率非常小。所以也不怎么使用它。</p><ul><li><h3 id="ReLU函数（线性修正单元）"><a href="#ReLU函数（线性修正单元）" class="headerlink" title="ReLU函数（线性修正单元）"></a>ReLU函数（线性修正单元）</h3><p>它的公式为 <strong>ReLU = max(0,z)</strong> , 图像长这样：</p><p><img src="https://i.loli.net/2020/03/31/A3i1mjGnDJQ58YV.png" style="zoom:50%;"></p></li></ul><p>所以只要z为正数，那么斜率就一直为1，z为负数时，斜率为0。虽然当z=0时，导数无定义（左导数和右导数不相等），但是在实际编程时，遇到z=0的概率非常非常小，并且也可以通过指定z=0时，导数为0或者1，所以不用纠结这个问题。</p><p>所以一般情况下都用<strong>ReLU</strong>，或者在你不知道使用哪种激活函数时，就选择它。就比如我们在做二元分类的问题时，可以将所有隐藏层的激活函数都设置为<strong>ReLU</strong>，然后将输出层的激活函数设置为<strong>sigmoid</strong>。</p><p>此外 ，ReLU函数还有另一种版本。</p><ul><li><h3 id="Leaky-ReLU（带泄露的ReLU）"><a href="#Leaky-ReLU（带泄露的ReLU）" class="headerlink" title="Leaky ReLU（带泄露的ReLU）"></a>Leaky ReLU（带泄露的ReLU）</h3></li></ul><p>它的公式为 <strong>ReLU = max(0.01z,z)</strong> , 图像长这样：</p><p><img src="https://i.loli.net/2020/03/31/YEVe8Xv9My4WQZ3.png" style="zoom:50%;"></p><p>当z为负数时，它的斜率不在为0，而是一个轻微的倾斜。这种激活函数实际上使用起来和ReLU没什么太大的区别，具体可以看个人爱好，或者在你的模型中，将两种都尝试一下，来试试效果。</p><p>顺带说一句，因为在NN中，有足够多的隐藏单元来使得z为正数，所以在使用ReLU的时候也不用太担心梯度下降的问题。</p><h2 id="激活函数求导"><a href="#激活函数求导" class="headerlink" title="激活函数求导"></a>激活函数求导</h2><ul><li><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><p>关于它的求导，可以参考上一篇博客：<a href="https://www.miraclesky.cn/neural-networks-deep-learning-week2.html" target="_blank" rel="noopener">neural-networks-deep-learning-week2</a></p><p>这里就不再赘述。</p></li><li><h3 id="tanh-函数"><a href="#tanh-函数" class="headerlink" title="tanh 函数"></a>tanh 函数</h3><p>首先，有它的公式</p><script type="math/tex; mode=display">g(z) = tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}</script></li></ul><p>  则有</p><script type="math/tex; mode=display">  g^{'}（z） = 1 - (tanh(z))^{2}</script><p>  具体求导过程可以自行求导，不是很难</p><ul><li><h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><script type="math/tex; mode=display">g(z) = max(0,z)</script><p>求导非常简单</p><script type="math/tex; mode=display">g^{'}(z) = \left\{ \begin{aligned}1 & &if && z\geq0 \\0 & &if && z<0\end{aligned}\right.</script><p>当z=0时，我们手动设置导数为0或者1（在数学中这是不可以的，==该点不可导==）</p></li></ul><h2 id="神经网络中的梯度下降"><a href="#神经网络中的梯度下降" class="headerlink" title="神经网络中的梯度下降"></a><font color="red">神经网络中的梯度下降</font></h2><p>现在需要讨论 在NN 中如何进行梯度下降，假设目前还是做得二元分类问题且只包含一层隐藏层，那么我们有损失函数的定义如下：</p><script type="math/tex; mode=display">J(w^{[1]},b^{[1]},w^{[2]},b^{[2]}) =\frac{1}{m}\sum_{i=1}^m  L(\hat{y}^\left(i\right),y^\left(i\right)) \\其中 ，n_x=n^{[0]}为特征的个数，n^{[1]}为隐藏单元个数，n^{[2]}为输出单元个数\\w^{[1]}的维度 为 n^{[1]} * n^{[0]}，b^{[1]}是一个n^{[1]}维向量，即n^{[1]}*1 \\w^{[1]}的维度 为 n^{[2]} * n^{[1]}，b^{[2]}是一个n^{[2]}维向量，即n^{[2]}*1</script><ul><li>梯度下降的做法就是用 变量 减去 学习率与对应偏导数的乘积</li></ul><script type="math/tex; mode=display">\begin{array}\\Repeat\{\\dw^{[1]} = \frac{\partial J}{\partial w^{[1]}}, db^{[1]} = \frac{\partial J}{\partial b^{[1]}} …… \\w^{[1]} = w^{[1]} - \alpha * dw^{[1]},b^{[1]} = b^{[1]} - \alpha * db^{[1]},……\\\}\end{array}</script><ul><li><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>具体的过程为（向量化）：</p></li></ul><script type="math/tex; mode=display">\begin{array}dZ^{[2]} = A^{[2]} - Y \\dW^{[2]} = \frac{1}{m}dZ^{[2]} * A^{[1]T} \\db^{[2]} =  \frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)\\axis=1为水平方向，keepdims=True防止输出秩为一的矩阵即(n,)这种形式 \\dZ^{[1]} = W^{[2]T}  dZ^{[2]} .* g^{[1]'}(Z^{[1]})              \\dW^{[1]} = \frac{1}{m} dZ^{[1]}X^{T}\\db^{[1]} = \frac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True)\\\end{array}</script>  <font color="red">（   .* 代表逐个元素相乘  ）</font><p>整个过程就是这样，如果不是很明白，可以去B站搜一搜，有很多推导过程，由于时间所限，这里就不一一推导。</p><h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>在逻辑回归中，将权重参数全部初始化为0这是可以的，但是在NN中这是不行的。我们来看一看这是为什么。</p><p><img src="https://i.loli.net/2020/03/31/FcsIWydzCPEST6k.png" style="zoom:50%;"></p><p>在上图中，有两个输入即<script type="math/tex">n^{[0]} =2</script>, 隐藏层有两个单元即<script type="math/tex">n^{[1]} =2</script>,那么<script type="math/tex">W^{[1]}</script>是一个2x2的矩阵，<script type="math/tex">b^{[1]}</script>是一个2x1的矩阵。而这会导致无论输入是什么，总有<script type="math/tex">a^{[1]}_1 =</script> <script type="math/tex">a^{[1]}_2</script>。当进行反向传播的时候，由于对称问题，会导致 <script type="math/tex">dz^{[1]}_1</script> = <script type="math/tex">dz^{[1]}_2</script></p><p>…… </p><p>因此，如果全部初始化为0的话，所以的隐藏单元实际上都是相同的，无论训练多久，它们始终相同。推广到多个隐藏层，隐藏单元也依旧成立。所以需要随机初始化。</p><hr><p>本周内容相对而言稍微难一点，主要是反向传播中的链式求导，不太熟的可以去看看链式求导法则。最好是要搞懂原理，不然在之后的多层网络中会比较难以理解</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>neural-networks-deep-learning-week2</title>
      <link href="/posts/3ee1b592.html"/>
      <url>/posts/3ee1b592.html</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="/js/baidu.js"></script><h1 id="Deep-Learning-将逻辑回归模型作为神经网络"><a href="#Deep-Learning-将逻辑回归模型作为神经网络" class="headerlink" title="[Deep Learning]将逻辑回归模型作为神经网络"></a>[Deep Learning]将逻辑回归模型作为神经网络</h1><p>最近开始学习吴恩达的deeplearning课程，从第二周开始，会开始介绍关于逻辑回归模型的一些简单定义，在这里开始做一些简单的笔记，用以后面的复习。</p><a id="more"></a><ul><li><h4 id="预测输出"><a href="#预测输出" class="headerlink" title="预测输出"></a>预测输出</h4></li></ul><script type="math/tex; mode=display">  \hat{y} = sigmoid(w^{T} + b)</script><script type="math/tex; mode=display">  sigmoid = \frac{1}{1 + e^{-z}}</script><ul><li><h4 id="损失函数（Loss）："><a href="#损失函数（Loss）：" class="headerlink" title="损失函数（Loss）："></a>损失函数（Loss）：</h4><p>对于线性回归中：</p><script type="math/tex; mode=display">L = \left(\dfrac{1}{2}\right) (\hat{y} - y)^{2}</script><p>而在逻辑回归中，损失函数被定义为：</p><script type="math/tex; mode=display">L = -(ylog(\hat{y}) + (1-y)log(1-\hat{y}))</script><p>具体而言，可以这么理解 yhat：</p><p>即在给定x的情况下 y等于1的概率为多少 </p><script type="math/tex; mode=display">\hat{y} = p(y=1|x)</script></li></ul><p>  也就是说 </p><p>  如果 y=1  </p><script type="math/tex; mode=display">  p(y|x) = \hat{y}</script><p>  如果 y=0</p><script type="math/tex; mode=display">  p(y|x) = 1 - \hat{y}</script><p>  将两种情况写在一起 即为  （将y=0 y=1 代入 可以得到与上面相同的公式）</p><script type="math/tex; mode=display">  p(y|x) = \hat{y}^{y}(1 - \hat{y})^{1 - y}</script><p>  又由于log函数为单调增函数，故取对数</p><script type="math/tex; mode=display">  log\left(p(y|x)\right) = ylog(\hat{y}) + (1-y)log(1-\hat{y}) = -L</script><p>  注意这里的负号，因为在逻辑回归中，我们想最大化概率，所以就需要最小化损失函数。</p><ul><li><h4 id="代价函数（COST）"><a href="#代价函数（COST）" class="headerlink" title="代价函数（COST）"></a>代价函数（COST）</h4><p>在m个样本中的代价可以定义为</p><script type="math/tex; mode=display">log(p) = log\prod_{i=1}^m p(y^\left(i\right)|x^\left(i\right)) = \sum_{i=1}^m log\left(p(y^\left(i\right)|x^\left(i\right))\right) = -\sum_{i=1}^m L(\hat{y}^\left(i\right),y^\left(i\right))</script><p>（在统计学中，有一种最大似然估计的方法，即选择使式子最大化的参数。）</p><p>则有 </p><script type="math/tex; mode=display">J(w,b) =\frac{1}{m}\sum_{i=1}^m  L(\hat{y}^\left(i\right),y^\left(i\right))</script><p>因为这里要求最小值，故不需要这个负号(1/m 为缩放系数，只是为了最后的数值能在更好的尺度上，没有其他特殊含义)</p></li></ul><ul><li><h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>在逻辑回归中，我们有以下的定义：</p><p><img src="https://i.loli.net/2020/03/28/P8XJSrQYeNp4ozO.png" alt></p><p>对其求偏导数（链式求导，如果不熟悉，可以看宇哥的十八讲~）</p><script type="math/tex; mode=display">\frac{dL}{da} = -\frac{y}{a} + \frac{1-y}{1-a}</script></li></ul><script type="math/tex; mode=display">  \frac{da}{dz} =-(1+e^{-z})^{-2} * -e^{-z} = e^{-z}*(1+e^{-z})^{-2} = a*(1-a)</script><p>注：</p><script type="math/tex; mode=display">e^{-z}*(1+e^{-z})^{-2} = \frac{e^{-z}}{(1+e^{-z})^{2}} = \frac{1+e^{-z}-1}{(1+e^{-z})^{2}} = \frac{1}{1+e^{-z}} - \frac{1}{(1+e^{-z})^{2}} = a-a^2=a(1-a)</script><p>则有</p><script type="math/tex; mode=display">\frac{dL}{dz} = \frac{dL}{da} * \frac{da}{dz} =(-\frac{y}{a} + \frac{1-y}{1-a}) * a(1-a)=a-y</script><p>同样的</p><script type="math/tex; mode=display">\frac{dL}{dw_1} = x_1*dz</script><p>在计算出这些导数后，就可以进行梯度下降了</p><script type="math/tex; mode=display">w_1 = w_1 - \alpha*dw_1</script><script type="math/tex; mode=display">w_2 = w_2 - \alpha*dw_2</script><script type="math/tex; mode=display">b = b -\alpha*db</script><hr><p>以上就是对第二周的小小整理（没有对向量化的部分以及numpy的使用做总结，关于numpy的使用会单独写），具体来说，这门课和之前的机器学习中讲的略有不同，比如记号之类的地方，但是大体而言还是差不多的，加油加油！</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
